{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d76100a",
   "metadata": {},
   "source": [
    "# READ ME (This is a supplemental notebook, main.ipynb is the full project)\n",
    "\n",
    "This script is **not meant to be run**, and the stacked time series file won't be provided, as its 50 GB of patient health information. We will use the features derived from this time series instead of the raw health data in the project. This script outlines how the channel labels were mapped to brain regions, the fourier transform to extract the alpha power present in our features.csv, among other inspection methods to better understand how to load in and work with the data (scipy matfile reader? h5py?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3d300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv with 185 rows and columns: ['name', 'cost', 'location', 'x', 'y']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "SRC = Path(r\"C:\\Users\\0218s\\Desktop\\channels_1_185_with_labels.csv\")\n",
    "DST = Path(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\")\n",
    "DST.parent.mkdir(parents=True, exist_ok=True)\n",
    "m = pd.read_csv(SRC)\n",
    "if \"label\" not in m.columns:\n",
    "    raise ValueError(f\"'label' column not found. Columns: {m.columns.tolist()}\")\n",
    "cols_lower = {c.lower(): c for c in m.columns}\n",
    "has_xy  = (\"x\" in cols_lower) and (\"y\" in cols_lower)\n",
    "has_xyz = {\"x\",\"y\",\"z\"}.issubset(cols_lower.keys())\n",
    "if has_xy:\n",
    "    x = pd.to_numeric(m[cols_lower[\"x\"]], errors=\"coerce\")\n",
    "    y = pd.to_numeric(m[cols_lower[\"y\"]], errors=\"coerce\")\n",
    "elif has_xyz:\n",
    "    xyz = m[[cols_lower[\"x\"], cols_lower[\"y\"], cols_lower[\"z\"]]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    arr = xyz.to_numpy(dtype=float)\n",
    "    norm = np.linalg.norm(arr, axis=1, keepdims=True)\n",
    "    norm[norm==0] = 1.0\n",
    "    arr = arr / norm\n",
    "    x = pd.Series(arr[:,0])\n",
    "    y = pd.Series(arr[:,1])\n",
    "else:\n",
    "    x = pd.Series([np.nan]*len(m))\n",
    "    y = pd.Series([np.nan]*len(m))\n",
    "def label_to_region(lbl: str) -> str:\n",
    "    s = lbl.strip().upper()\n",
    "    # Common \n",
    "    if s.startswith(\"FP\"): return \"frontopolar\"\n",
    "    if s.startswith(\"AF\"): return \"frontal (anterior)\"\n",
    "    if s.startswith(\"FA\"): return \"frontal (anterior)\"\n",
    "    if s.startswith(\"FC\"): return \"fronto-central\"\n",
    "    if s.startswith(\"FT\"): return \"fronto-temporal\"\n",
    "    if s.startswith(\"CP\"): return \"centro-parietal\"\n",
    "    if s.startswith(\"PO\"): return \"parieto-occipital\"\n",
    "    if s.startswith(\"TP\"): return \"temporo-parietal\"\n",
    "    if s.startswith(\"OP\"): return \"occipito-parietal\"\n",
    "    if s.startswith(\"O\"):  return \"occipital\"\n",
    "    if s.startswith(\"P\"):  return \"parietal\"\n",
    "    if s.startswith(\"C\"):  return \"central\"\n",
    "    if s.startswith(\"F\"):  return \"frontal\"\n",
    "    if s.startswith(\"T\"):  return \"temporal\"\n",
    "    # High-density suffixes (h) don’t change lobe\n",
    "    if re.match(r\".*H$\", s):\n",
    "        base = re.sub(r\"H$\", \"\", s)\n",
    "        return label_to_region(base)\n",
    "    return \"unknown\"\n",
    "names = m[\"label\"].astype(str)\n",
    "regions = names.map(label_to_region)\n",
    "upper = names.str.upper()\n",
    "cost = pd.Series(1.0, index=names.index)\n",
    "mask_fp = upper.str.startswith(\"FP\")\n",
    "mask_inf_temp = upper.isin([\"T3\",\"T4\",\"T7\",\"T8\"])  # handle both naming conventions\n",
    "cost.loc[mask_fp | mask_inf_temp] = 1.5\n",
    "out = pd.DataFrame({\n",
    "    \"name\": names,\n",
    "    \"cost\": cost.values,\n",
    "    \"location\": regions.values,\n",
    "    \"x\": x.values,\n",
    "    \"y\": y.values,\n",
    "})\n",
    "out = out.drop_duplicates(subset=[\"name\"]).sort_values(\"name\").reset_index(drop=True)\n",
    "out.to_csv(DST, index=False)\n",
    "print(f\"Wrote {DST} with {len(out)} rows and columns: {list(out.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated locations/costs written.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\")\n",
    "mask_afp = df[\"name\"].str.upper().str.startswith(\"AFP\")\n",
    "df.loc[mask_afp, \"location\"] = \"frontopolar\"\n",
    "upper = df[\"name\"].str.upper()\n",
    "hard = (\n",
    "    upper.str.startswith(\"FP\") |       # FP1, FP2, FPz...\n",
    "    upper.str.startswith(\"AFP\") |      # AFp3, AFpz...\n",
    "    upper.isin([\"T7\",\"T8\",\"T3\",\"T4\"]) |   # temporal legacy/new\n",
    "    upper.isin([\"FT7\",\"FT8\",\"TP7\",\"TP8\",\"A1\",\"A2\"])\n",
    ")\n",
    "df[\"cost\"] = 1.0\n",
    "df.loc[hard, \"cost\"] = 1.5\n",
    "df.to_csv(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\", index=False)\n",
    "print(\"Updated locations/costs written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/usefulness_demo.csv\n"
     ]
    }
   ],
   "source": [
    "# test with surrogate data 1st (not in main.ipynb deliverable)\n",
    "import pandas as pd, numpy as np\n",
    "elec = pd.read_csv(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\")  # expects columns: name,location,…\n",
    "subjects = [f\"S{i:02d}\" for i in range(1, 11)]  # 10 demo subjects\n",
    "occ = elec[\"location\"].str.contains(\"occipital\", case=False, na=False)\n",
    "par = elec[\"location\"].str.contains(\"parietal\",  case=False, na=False)\n",
    "posterior_mask = (occ | par).to_numpy(dtype=float)\n",
    "rng = np.random.default_rng(42)\n",
    "rows = []\n",
    "for s in subjects:\n",
    "    base = rng.uniform(0.20, 0.50, size=len(elec))                    # baseline\n",
    "    bump = rng.uniform(0.05, 0.15, size=len(elec)) * posterior_mask   # posterior lift\n",
    "    vals = (base + bump).clip(0, 1)\n",
    "    rows += [{\"subject\": s, \"electrode\": name, \"usefulness\": float(v)}\n",
    "             for name, v in zip(elec[\"name\"], vals)]\n",
    "pd.DataFrame(rows).to_csv(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\usefulness_demo.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c285cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SciPy loadmat (v5/v7) ===\n",
      "SciPy loadmat fallback: Please use HDF reader for matlab v7.3 files, e.g. h5py\n",
      "\n",
      "=== h5py (v7.3/HDF5) ===\n",
      "- stackedEEG_noreport_60s   | h5py.Dataset[float64]               | shape=(358, 20, 1500, 185)\n",
      "- stackedEEG_report_60s     | h5py.Dataset[float64]               | shape=(341, 20, 1500, 185)\n",
      "- stackedEEG_something_60s  | h5py.Dataset[float64]               | shape=(355, 20, 1500, 185)\n"
     ]
    }
   ],
   "source": [
    "# reader formats: AI Note: This code snippet is generated by ChatGPT to test different methods of loading in the time series\n",
    "import os, numpy as np\n",
    "path = r\"C:\\Users\\0218s\\Desktop\\stack03312023.mat\" \n",
    "def pretty_shape(x):\n",
    "    try:\n",
    "        return tuple(x.shape)\n",
    "    except Exception:\n",
    "        return None\n",
    "def try_scipy(p):\n",
    "    import scipy.io as sio\n",
    "    d = sio.loadmat(p, struct_as_record=False, squeeze_me=False)\n",
    "    keys = [k for k in d.keys() if not k.startswith(\"__\")]\n",
    "    rows = []\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        t = type(v).__name__\n",
    "        shp = pretty_shape(v)\n",
    "        if isinstance(v, np.ndarray) and getattr(v.dtype, \"names\", None):\n",
    "            t = f\"np.ndarray(struct dtype={v.dtype.names})\"\n",
    "        rows.append((k, t, shp))\n",
    "    return rows\n",
    "def try_h5py(p):\n",
    "    import h5py\n",
    "    rows = []\n",
    "    with h5py.File(p, \"r\") as f:\n",
    "        for k in f.keys():\n",
    "            obj = f[k]\n",
    "            if isinstance(obj, h5py.Dataset):\n",
    "                rows.append((k, f\"h5py.Dataset[{obj.dtype}]\", tuple(obj.shape)))\n",
    "            else:\n",
    "                rows.append((k, \"h5py.Group\", None))\n",
    "    return rows\n",
    "if not os.path.exists(path):\n",
    "    print(f\"File not found: {path}\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"=== SciPy loadmat (v5/v7) ===\")\n",
    "        for k, t, shp in try_scipy(path):\n",
    "            print(f\"- {k:25s} | {t:35s} | shape={shp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"SciPy loadmat fallback: {e}\")\n",
    "        try:\n",
    "            print(\"\\n=== h5py (v7.3/HDF5) ===\")\n",
    "            for k, t, shp in try_h5py(path):\n",
    "                print(f\"- {k:25s} | {t:35s} | shape={shp}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"HDF5 fallback: {e2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb9d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] stackedEEG_report_60s: N=341, segments=20, samples/seg=1500, channels=185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0218s\\AppData\\Local\\Temp\\ipykernel_17700\\1747744793.py:27: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return float(np.trapz(psd[idx], freqs[idx])) if np.any(idx) else 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] stackedEEG_noreport_60s: N=358, segments=20, samples/seg=1500, channels=185\n",
      "[DONE] C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\features.csv  subjects=699  electrodes=185  rows=129315\n"
     ]
    }
   ],
   "source": [
    "import h5py, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "MAT_PATH   = r\"C:\\Users\\0218s\\Desktop\\stack03312023.mat\"\n",
    "MAP_CSV    = r\"C:\\Users\\0218s\\Desktop\\channels_1_185_with_labels.csv\"  # has Channel,label\n",
    "ELEC_CSV   = r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\"\n",
    "OUT_CSV    = r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\features.csv\"\n",
    "DATASETS = {\n",
    "    \"stackedEEG_report_60s\":    1,  # CE\n",
    "    \"stackedEEG_noreport_60s\":  0,  # NCE\n",
    "}\n",
    "FS = 500.0\n",
    "WIN_SEC, NOVERLAP = 2.0, 0.5\n",
    "NPERSEG = int(WIN_SEC * FS)\n",
    "NOVERLAP_SAMPLES = int(NPERSEG * NOVERLAP)\n",
    "BANDS = {\"delta\":(0.5,4.0),\"theta\":(4.0,8.0),\"alpha\":(8.0,12.0),\"sigma\":(12.0,16.0),\"beta\":(16.0,30.0)}\n",
    "LOW, HIGH = 0.5, 30.0\n",
    "def bandpower_from_psd(freqs, psd, fmin, fmax):\n",
    "    idx = (freqs >= fmin) & (freqs < fmax)\n",
    "    return float(np.trapz(psd[idx], freqs[idx])) if np.any(idx) else 0.0\n",
    "def rel_alpha_power(x_1d):\n",
    "    f, Pxx = welch(x_1d, fs=FS, nperseg=NPERSEG, noverlap=NOVERLAP_SAMPLES, scaling=\"density\")\n",
    "    p_tot  = bandpower_from_psd(f, Pxx, LOW, HIGH) + 1e-12\n",
    "    p_alph = bandpower_from_psd(f, Pxx, *BANDS[\"alpha\"])\n",
    "    return float(p_alph / p_tot)\n",
    "def main():\n",
    "    m = pd.read_csv(MAP_CSV)  \n",
    "    if \"Channel\" not in m.columns or \"label\" not in m.columns:\n",
    "        raise ValueError(f\"{MAP_CSV} must have columns 'Channel' and 'label'\")\n",
    "    m[\"label_norm\"] = m[\"label\"].astype(str).str.strip()\n",
    "    idx2label = dict(zip(m[\"Channel\"].astype(int), m[\"label_norm\"]))\n",
    "    elec = pd.read_csv(ELEC_CSV)\n",
    "    have = set(elec[\"name\"].astype(str).str.strip())\n",
    "    missing = sorted({lbl for lbl in idx2label.values() if lbl not in have})\n",
    "    if missing:\n",
    "        print(f\"[WARN] {len(missing)} labels in mapping not found in electrodes.csv (showing up to 10): {missing[:10]}\")\n",
    "    rows = []\n",
    "    with h5py.File(MAT_PATH, \"r\") as f:\n",
    "        for ds_name, label in DATASETS.items():\n",
    "            if ds_name not in f:\n",
    "                print(f\"[WARN] dataset {ds_name} not found; skipping\")\n",
    "                continue\n",
    "            dset = f[ds_name]  # shape = (N, 20, 1500, 185)\n",
    "            N, S, T, C = dset.shape\n",
    "            assert C == 185, f\"Expected 185 channels, got {C}\"\n",
    "            assert S*T == int(60*FS), f\"Expected 60s total: S*T={S*T} vs 60*FS={60*FS}\"\n",
    "            print(f\"[INFO] {ds_name}: N={N}, segments={S}, samples/seg={T}, channels={C}\")\n",
    "            for n in range(N):\n",
    "                x = dset[n]  # (S, T, C)\n",
    "                feat_seg = np.zeros((S, C), dtype=np.float32)\n",
    "                for s in range(S):\n",
    "                    seg = x[s]  # (T, C)\n",
    "                    for c in range(C):\n",
    "                        feat_seg[s, c] = rel_alpha_power(seg[:, c])\n",
    "                feat_ch = feat_seg.mean(axis=0)  # (C,)\n",
    "                subj_id = f\"{ds_name}_S{n:03d}\"\n",
    "                for ci in range(C):\n",
    "                    ch_idx_mat = ci + 1\n",
    "                    name = idx2label.get(ch_idx_mat, f\"CH{ch_idx_mat}\")\n",
    "                    rows.append({\n",
    "                        \"subject\":   subj_id,\n",
    "                        \"electrode\": name,\n",
    "                        \"feature\":   float(feat_ch[ci]),\n",
    "                        \"label\":     int(label),\n",
    "                    })\n",
    "    out = pd.DataFrame(rows)\n",
    "    Path(OUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"[DONE] {OUT_CSV}  subjects={out['subject'].nunique()}  electrodes={out['electrode'].nunique()}  rows={len(out)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c6bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\usefulness_real.csv  rows=129315  subjects=699  electrodes=185\n",
      "Example:\n",
      "                        subject electrode  usefulness\n",
      "0  stackedEEG_noreport_60s_S000      AF1h       0.127\n",
      "1  stackedEEG_noreport_60s_S001      AF1h       0.149\n",
      "2  stackedEEG_noreport_60s_S002      AF1h       0.097\n",
      "3  stackedEEG_noreport_60s_S003      AF1h       0.005\n",
      "4  stackedEEG_noreport_60s_S004      AF1h       0.124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "FEATURES_PATH   = Path(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\features.csv\")\n",
    "ELECTRODES_PATH = Path(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\")\n",
    "OUT_PATH        = Path(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\usefulness_real.csv\")\n",
    "NORMALIZE_PER_SUBJECT = True  \n",
    "SEED = 42\n",
    "MAX_ITER = 300\n",
    "def main():\n",
    "    if not FEATURES_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Missing {FEATURES_PATH}. Create it from your .mat first.\")\n",
    "    if not ELECTRODES_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Missing {ELECTRODES_PATH} (expects column 'name').\")\n",
    "    df = pd.read_csv(FEATURES_PATH)       # subject,electrode,feature,label\n",
    "    elec = pd.read_csv(ELECTRODES_PATH)   # name,cost,location,x,y ...\n",
    "    E = elec[\"name\"].astype(str).tolist()\n",
    "    X = df.pivot_table(index=\"subject\", columns=\"electrode\", values=\"feature\", aggfunc=\"mean\")\n",
    "    y = df.drop_duplicates(\"subject\").set_index(\"subject\")[\"label\"].reindex(X.index)\n",
    "    X = X.reindex(columns=E).fillna(0.0)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_std = scaler.fit_transform(X.to_numpy())\n",
    "    clf = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"liblinear\",\n",
    "        class_weight=\"balanced\",   \n",
    "        max_iter=MAX_ITER,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    clf.fit(X_std, y.to_numpy())\n",
    "    w = clf.coef_.ravel()  # aligned with X columns\n",
    "    Xstd_df = pd.DataFrame(X_std, index=X.index, columns=X.columns)\n",
    "    contrib = (Xstd_df * w).abs()\n",
    "    if NORMALIZE_PER_SUBJECT:\n",
    "        mins = contrib.min(axis=1)\n",
    "        denom = (contrib.max(axis=1) - mins + 1e-12)\n",
    "        usefulness = contrib.sub(mins, axis=0).div(denom, axis=0)\n",
    "    else:\n",
    "        lo, hi = contrib.values.min(), contrib.values.max()\n",
    "        usefulness = (contrib - lo) / (hi - lo + 1e-12)\n",
    "    out = usefulness.reset_index().melt(id_vars=\"subject\",\n",
    "                                        var_name=\"electrode\",\n",
    "                                        value_name=\"usefulness\")\n",
    "    out[\"usefulness\"] = out[\"usefulness\"].clip(0, 1).round(3)\n",
    "    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(OUT_PATH, index=False)\n",
    "    print(f\"Wrote {OUT_PATH}  rows={len(out)}  subjects={out['subject'].nunique()}  electrodes={out['electrode'].nunique()}\")\n",
    "    print(\"Example:\")\n",
    "    print(out.head())\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
