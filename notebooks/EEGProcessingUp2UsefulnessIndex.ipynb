{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fb3d300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv with 185 rows and columns: ['name', 'cost', 'location', 'x', 'y']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# === Your paths ===\n",
    "SRC = Path(r\"C:\\Users\\0218s\\Desktop\\channels_1_185_with_labels.csv\")\n",
    "DST = Path(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\")\n",
    "DST.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "m = pd.read_csv(SRC)\n",
    "\n",
    "# --- Force name = label (NOT Channel) ---\n",
    "if \"label\" not in m.columns:\n",
    "    raise ValueError(f\"'label' column not found. Columns: {m.columns.tolist()}\")\n",
    "\n",
    "# --- Get coordinates: prefer x,y; else project x,y,z -> 2D ---\n",
    "cols_lower = {c.lower(): c for c in m.columns}\n",
    "has_xy  = (\"x\" in cols_lower) and (\"y\" in cols_lower)\n",
    "has_xyz = {\"x\",\"y\",\"z\"}.issubset(cols_lower.keys())\n",
    "\n",
    "if has_xy:\n",
    "    x = pd.to_numeric(m[cols_lower[\"x\"]], errors=\"coerce\")\n",
    "    y = pd.to_numeric(m[cols_lower[\"y\"]], errors=\"coerce\")\n",
    "elif has_xyz:\n",
    "    xyz = m[[cols_lower[\"x\"], cols_lower[\"y\"], cols_lower[\"z\"]]].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    arr = xyz.to_numpy(dtype=float)\n",
    "    norm = np.linalg.norm(arr, axis=1, keepdims=True)\n",
    "    norm[norm==0] = 1.0\n",
    "    arr = arr / norm\n",
    "    x = pd.Series(arr[:,0])\n",
    "    y = pd.Series(arr[:,1])\n",
    "else:\n",
    "    x = pd.Series([np.nan]*len(m))\n",
    "    y = pd.Series([np.nan]*len(m))\n",
    "\n",
    "# --- Derive a simple region/location from the label ---\n",
    "def label_to_region(lbl: str) -> str:\n",
    "    s = lbl.strip().upper()\n",
    "    # Common composites first\n",
    "    if s.startswith(\"FP\"): return \"frontopolar\"\n",
    "    if s.startswith(\"AF\"): return \"frontal (anterior)\"\n",
    "    if s.startswith(\"FA\"): return \"frontal (anterior)\"\n",
    "    if s.startswith(\"FC\"): return \"fronto-central\"\n",
    "    if s.startswith(\"FT\"): return \"fronto-temporal\"\n",
    "    if s.startswith(\"CP\"): return \"centro-parietal\"\n",
    "    if s.startswith(\"PO\"): return \"parieto-occipital\"\n",
    "    if s.startswith(\"TP\"): return \"temporo-parietal\"\n",
    "    if s.startswith(\"OP\"): return \"occipito-parietal\"\n",
    "    if s.startswith(\"O\"):  return \"occipital\"\n",
    "    if s.startswith(\"P\"):  return \"parietal\"\n",
    "    if s.startswith(\"C\"):  return \"central\"\n",
    "    if s.startswith(\"F\"):  return \"frontal\"\n",
    "    if s.startswith(\"T\"):  return \"temporal\"\n",
    "    # High-density suffixes (h) don’t change lobe\n",
    "    if re.match(r\".*H$\", s):  # e.g., F3h\n",
    "        base = re.sub(r\"H$\", \"\", s)\n",
    "        return label_to_region(base)\n",
    "    return \"unknown\"\n",
    "\n",
    "names = m[\"label\"].astype(str)\n",
    "regions = names.map(label_to_region)\n",
    "\n",
    "# --- Optional: bump cost for tricky sites (customize as you like) ---\n",
    "upper = names.str.upper()\n",
    "cost = pd.Series(1.0, index=names.index)\n",
    "mask_fp = upper.str.startswith(\"FP\")\n",
    "mask_inf_temp = upper.isin([\"T3\",\"T4\",\"T7\",\"T8\"])  # handle both naming conventions\n",
    "cost.loc[mask_fp | mask_inf_temp] = 1.5\n",
    "\n",
    "# --- Build and save electrodes.csv ---\n",
    "out = pd.DataFrame({\n",
    "    \"name\": names,\n",
    "    \"cost\": cost.values,\n",
    "    \"location\": regions.values,\n",
    "    \"x\": x.values,\n",
    "    \"y\": y.values,\n",
    "})\n",
    "\n",
    "# Deduplicate and sort by name\n",
    "out = out.drop_duplicates(subset=[\"name\"]).sort_values(\"name\").reset_index(drop=True)\n",
    "out.to_csv(DST, index=False)\n",
    "print(f\"Wrote {DST} with {len(out)} rows and columns: {list(out.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea0b4e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated locations/costs written.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\")\n",
    "\n",
    "# Fix region: AFp* -> frontopolar\n",
    "mask_afp = df[\"name\"].str.upper().str.startswith(\"AFP\")\n",
    "df.loc[mask_afp, \"location\"] = \"frontopolar\"\n",
    "\n",
    "# Two-tier costs\n",
    "upper = df[\"name\"].str.upper()\n",
    "hard = (\n",
    "    upper.str.startswith(\"FP\") |       # FP1, FP2, FPz...\n",
    "    upper.str.startswith(\"AFP\") |      # AFp3, AFpz...\n",
    "    upper.isin([\"T7\",\"T8\",\"T3\",\"T4\"]) |   # temporal legacy/new\n",
    "    upper.isin([\"FT7\",\"FT8\",\"TP7\",\"TP8\",\"A1\",\"A2\"])\n",
    ")\n",
    "df[\"cost\"] = 1.0\n",
    "df.loc[hard, \"cost\"] = 1.5\n",
    "\n",
    "df.to_csv(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\", index=False)\n",
    "print(\"Updated locations/costs written.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "358eb06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 185  Unique names: 185\n",
      "Any NaNs? {'name': 0, 'cost': 0, 'location': 0, 'x': 0, 'y': 0}\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\0218s\\\\Desktop\\\\optimal-electrode-mip\\\\data\\\\electrodes.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m df.loc[mask_iz,  \u001b[33m'\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33moccipital (inion)\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     17\u001b[39m df.loc[mask_i12, \u001b[33m'\u001b[39m\u001b[33mlocation\u001b[39m\u001b[33m'\u001b[39m] = \u001b[33m'\u001b[39m\u001b[33moccipital (inferior)\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43m0218s\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDesktop\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43moptimal-electrode-mip\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43melectrodes.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mElectrodes file validated/updated.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0218s\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0218s\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3956\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3958\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3959\u001b[39m     frame=df,\n\u001b[32m   3960\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3964\u001b[39m     decimal=decimal,\n\u001b[32m   3965\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3972\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3981\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3982\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3983\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3984\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0218s\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0218s\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\0218s\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: 'C:\\\\Users\\\\0218s\\\\Desktop\\\\optimal-electrode-mip\\\\data\\\\electrodes.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\")\n",
    "\n",
    "print(\"Rows:\", len(df), \" Unique names:\", df['name'].nunique())\n",
    "print(\"Any NaNs?\", df.isna().sum().to_dict())\n",
    "\n",
    "# Show any duplicate names\n",
    "dups = df[df.duplicated('name', keep=False)].sort_values('name')\n",
    "if len(dups):\n",
    "    print(\"\\nDUPLICATES:\\n\", dups[['name','x','y']].to_string(index=False))\n",
    "\n",
    "# Optional: upgrade I* region labels\n",
    "mask_iz   = df['name'].str.upper().isin(['IZ','OIZ'])\n",
    "mask_i12  = df['name'].str.upper().isin(['I1','I2','I1H','I2H'])\n",
    "df.loc[mask_iz,  'location'] = 'occipital (inion)'\n",
    "df.loc[mask_i12, 'location'] = 'occipital (inferior)'\n",
    "\n",
    "df.to_csv(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\", index=False)\n",
    "print(\"Electrodes file validated/updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0eb681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote data/usefulness_demo.csv\n"
     ]
    }
   ],
   "source": [
    "# make_usefulness_demo.py\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "elec = pd.read_csv(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\")  # expects columns: name,location,…\n",
    "subjects = [f\"S{i:02d}\" for i in range(1, 11)]  # 10 demo subjects\n",
    "\n",
    "occ = elec[\"location\"].str.contains(\"occipital\", case=False, na=False)\n",
    "par = elec[\"location\"].str.contains(\"parietal\",  case=False, na=False)\n",
    "posterior_mask = (occ | par).to_numpy(dtype=float)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "rows = []\n",
    "for s in subjects:\n",
    "    base = rng.uniform(0.20, 0.50, size=len(elec))                    # baseline\n",
    "    bump = rng.uniform(0.05, 0.15, size=len(elec)) * posterior_mask   # posterior lift\n",
    "    vals = (base + bump).clip(0, 1)\n",
    "    rows += [{\"subject\": s, \"electrode\": name, \"usefulness\": float(v)}\n",
    "             for name, v in zip(elec[\"name\"], vals)]\n",
    "\n",
    "pd.DataFrame(rows).to_csv(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\usefulness_demo.csv\", index=False)\n",
    "print(\"Wrote data/usefulness_demo.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0c285cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SciPy loadmat (v5/v7) ===\n",
      "SciPy loadmat fallback: Please use HDF reader for matlab v7.3 files, e.g. h5py\n",
      "\n",
      "=== h5py (v7.3/HDF5) ===\n",
      "- stackedEEG_noreport_60s   | h5py.Dataset[float64]               | shape=(358, 20, 1500, 185)\n",
      "- stackedEEG_report_60s     | h5py.Dataset[float64]               | shape=(341, 20, 1500, 185)\n",
      "- stackedEEG_something_60s  | h5py.Dataset[float64]               | shape=(355, 20, 1500, 185)\n"
     ]
    }
   ],
   "source": [
    "# inspect_mat_hardcoded.py\n",
    "import os, numpy as np\n",
    "\n",
    "path = r\"C:\\Users\\0218s\\Desktop\\stack03312023.mat\"  # <-- your file\n",
    "\n",
    "def pretty_shape(x):\n",
    "    try:\n",
    "        return tuple(x.shape)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def try_scipy(p):\n",
    "    import scipy.io as sio\n",
    "    d = sio.loadmat(p, struct_as_record=False, squeeze_me=False)\n",
    "    keys = [k for k in d.keys() if not k.startswith(\"__\")]\n",
    "    rows = []\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        t = type(v).__name__\n",
    "        shp = pretty_shape(v)\n",
    "        if isinstance(v, np.ndarray) and getattr(v.dtype, \"names\", None):\n",
    "            t = f\"np.ndarray(struct dtype={v.dtype.names})\"\n",
    "        rows.append((k, t, shp))\n",
    "    return rows\n",
    "\n",
    "def try_h5py(p):\n",
    "    import h5py\n",
    "    rows = []\n",
    "    with h5py.File(p, \"r\") as f:\n",
    "        for k in f.keys():\n",
    "            obj = f[k]\n",
    "            if isinstance(obj, h5py.Dataset):\n",
    "                rows.append((k, f\"h5py.Dataset[{obj.dtype}]\", tuple(obj.shape)))\n",
    "            else:\n",
    "                rows.append((k, \"h5py.Group\", None))\n",
    "    return rows\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    print(f\"File not found: {path}\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"=== SciPy loadmat (v5/v7) ===\")\n",
    "        for k, t, shp in try_scipy(path):\n",
    "            print(f\"- {k:25s} | {t:35s} | shape={shp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"SciPy loadmat fallback: {e}\")\n",
    "        try:\n",
    "            print(\"\\n=== h5py (v7.3/HDF5) ===\")\n",
    "            for k, t, shp in try_h5py(path):\n",
    "                print(f\"- {k:25s} | {t:35s} | shape={shp}\")\n",
    "        except Exception as e2:\n",
    "            print(f\"HDF5 fallback: {e2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43fb9d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] stackedEEG_report_60s: N=341, segments=20, samples/seg=1500, channels=185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0218s\\AppData\\Local\\Temp\\ipykernel_17700\\1747744793.py:27: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return float(np.trapz(psd[idx], freqs[idx])) if np.any(idx) else 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] stackedEEG_noreport_60s: N=358, segments=20, samples/seg=1500, channels=185\n",
      "[DONE] C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\features.csv  subjects=699  electrodes=185  rows=129315\n"
     ]
    }
   ],
   "source": [
    "# make_features_from_mat.py\n",
    "import h5py, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "\n",
    "MAT_PATH   = r\"C:\\Users\\0218s\\Desktop\\stack03312023.mat\"\n",
    "MAP_CSV    = r\"C:\\Users\\0218s\\Desktop\\channels_1_185_with_labels.csv\"  # has Channel,label\n",
    "ELEC_CSV   = r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\"\n",
    "OUT_CSV    = r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\features.csv\"\n",
    "\n",
    "DATASETS = {\n",
    "    \"stackedEEG_report_60s\":    1,  # CE\n",
    "    \"stackedEEG_noreport_60s\":  0,  # NCE\n",
    "    # \"stackedEEG_something_60s\": ?,\n",
    "}\n",
    "\n",
    "FS = 500.0\n",
    "WIN_SEC, NOVERLAP = 2.0, 0.5\n",
    "NPERSEG = int(WIN_SEC * FS)\n",
    "NOVERLAP_SAMPLES = int(NPERSEG * NOVERLAP)\n",
    "\n",
    "BANDS = {\"delta\":(0.5,4.0),\"theta\":(4.0,8.0),\"alpha\":(8.0,12.0),\"sigma\":(12.0,16.0),\"beta\":(16.0,30.0)}\n",
    "LOW, HIGH = 0.5, 30.0\n",
    "\n",
    "def bandpower_from_psd(freqs, psd, fmin, fmax):\n",
    "    idx = (freqs >= fmin) & (freqs < fmax)\n",
    "    return float(np.trapz(psd[idx], freqs[idx])) if np.any(idx) else 0.0\n",
    "\n",
    "def rel_alpha_power(x_1d):\n",
    "    f, Pxx = welch(x_1d, fs=FS, nperseg=NPERSEG, noverlap=NOVERLAP_SAMPLES, scaling=\"density\")\n",
    "    p_tot  = bandpower_from_psd(f, Pxx, LOW, HIGH) + 1e-12\n",
    "    p_alph = bandpower_from_psd(f, Pxx, *BANDS[\"alpha\"])\n",
    "    return float(p_alph / p_tot)\n",
    "\n",
    "def main():\n",
    "    # --- load index→label map and the canonical electrode list for cross-check ---\n",
    "    m = pd.read_csv(MAP_CSV)  # expects columns: Channel,label (your file does)\n",
    "    if \"Channel\" not in m.columns or \"label\" not in m.columns:\n",
    "        raise ValueError(f\"{MAP_CSV} must have columns 'Channel' and 'label'\")\n",
    "\n",
    "    # normalize labels (match your electrodes.csv naming exactly)\n",
    "    m[\"label_norm\"] = m[\"label\"].astype(str).str.strip()\n",
    "    idx2label = dict(zip(m[\"Channel\"].astype(int), m[\"label_norm\"]))\n",
    "\n",
    "    elec = pd.read_csv(ELEC_CSV)\n",
    "    have = set(elec[\"name\"].astype(str).str.strip())\n",
    "\n",
    "    # sanity: all mapped labels should be present in electrodes.csv\n",
    "    missing = sorted({lbl for lbl in idx2label.values() if lbl not in have})\n",
    "    if missing:\n",
    "        print(f\"[WARN] {len(missing)} labels in mapping not found in electrodes.csv (showing up to 10): {missing[:10]}\")\n",
    "\n",
    "    rows = []\n",
    "    with h5py.File(MAT_PATH, \"r\") as f:\n",
    "        for ds_name, label in DATASETS.items():\n",
    "            if ds_name not in f:\n",
    "                print(f\"[WARN] dataset {ds_name} not found; skipping\")\n",
    "                continue\n",
    "            dset = f[ds_name]  # shape = (N, 20, 1500, 185)\n",
    "            N, S, T, C = dset.shape\n",
    "            assert C == 185, f\"Expected 185 channels, got {C}\"\n",
    "            assert S*T == int(60*FS), f\"Expected 60s total: S*T={S*T} vs 60*FS={60*FS}\"\n",
    "\n",
    "            print(f\"[INFO] {ds_name}: N={N}, segments={S}, samples/seg={T}, channels={C}\")\n",
    "\n",
    "            for n in range(N):\n",
    "                x = dset[n]  # (S, T, C)\n",
    "                # --- Option: segment-average PSD feature (robust) ---\n",
    "                feat_seg = np.zeros((S, C), dtype=np.float32)\n",
    "                for s in range(S):\n",
    "                    seg = x[s]  # (T, C)\n",
    "                    for c in range(C):\n",
    "                        feat_seg[s, c] = rel_alpha_power(seg[:, c])\n",
    "                feat_ch = feat_seg.mean(axis=0)  # (C,)\n",
    "\n",
    "                subj_id = f\"{ds_name}_S{n:03d}\"\n",
    "                # channel indices in the .mat are 0..184 → +1 to map to 1..185\n",
    "                for ci in range(C):\n",
    "                    ch_idx_mat = ci + 1\n",
    "                    name = idx2label.get(ch_idx_mat, f\"CH{ch_idx_mat}\")\n",
    "                    rows.append({\n",
    "                        \"subject\":   subj_id,\n",
    "                        \"electrode\": name,\n",
    "                        \"feature\":   float(feat_ch[ci]),\n",
    "                        \"label\":     int(label),\n",
    "                    })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    Path(OUT_CSV).parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(OUT_CSV, index=False)\n",
    "    print(f\"[DONE] {OUT_CSV}  subjects={out['subject'].nunique()}  electrodes={out['electrode'].nunique()}  rows={len(out)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "986c6bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\usefulness_real.csv  rows=129315  subjects=699  electrodes=185\n",
      "Example:\n",
      "                        subject electrode  usefulness\n",
      "0  stackedEEG_noreport_60s_S000      AF1h       0.127\n",
      "1  stackedEEG_noreport_60s_S001      AF1h       0.149\n",
      "2  stackedEEG_noreport_60s_S002      AF1h       0.097\n",
      "3  stackedEEG_noreport_60s_S003      AF1h       0.005\n",
      "4  stackedEEG_noreport_60s_S004      AF1h       0.124\n"
     ]
    }
   ],
   "source": [
    "# make_usefulness_real.py\n",
    "# Create per-(subject, electrode) usefulness scores from features via the Contribution method.\n",
    "# Input : data/features.csv  (columns: subject,electrode,feature,label)\n",
    "# Input : data/electrodes.csv (to align electrode order; column: name)\n",
    "# Output: data/usefulness_real.csv (columns: subject,electrode,usefulness) with values in [0,1]\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "FEATURES_PATH   = Path(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\features.csv\")\n",
    "ELECTRODES_PATH = Path(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\electrodes.csv\")\n",
    "OUT_PATH        = Path(r\"C:\\Users\\0218s\\Desktop\\optimal-electrode-mip\\data\\usefulness_real.csv\")\n",
    "\n",
    "# Config\n",
    "NORMALIZE_PER_SUBJECT = True   # True: min–max per subject; False: global min–max\n",
    "SEED = 42\n",
    "MAX_ITER = 300\n",
    "\n",
    "def main():\n",
    "    if not FEATURES_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Missing {FEATURES_PATH}. Create it from your .mat first.\")\n",
    "    if not ELECTRODES_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Missing {ELECTRODES_PATH} (expects column 'name').\")\n",
    "\n",
    "    # ---- Load data ----\n",
    "    df = pd.read_csv(FEATURES_PATH)       # subject,electrode,feature,label\n",
    "    elec = pd.read_csv(ELECTRODES_PATH)   # name,cost,location,x,y ...\n",
    "    E = elec[\"name\"].astype(str).tolist()\n",
    "\n",
    "    # Pivot to subjects x electrodes\n",
    "    X = df.pivot_table(index=\"subject\", columns=\"electrode\", values=\"feature\", aggfunc=\"mean\")\n",
    "    y = df.drop_duplicates(\"subject\").set_index(\"subject\")[\"label\"].reindex(X.index)\n",
    "\n",
    "    # Align to canonical electrode order; fill missing with 0\n",
    "    X = X.reindex(columns=E).fillna(0.0)\n",
    "\n",
    "    # ---- Standardize + fit logistic ----\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    X_std = scaler.fit_transform(X.to_numpy())\n",
    "\n",
    "    clf = LogisticRegression(\n",
    "        penalty=\"l2\",\n",
    "        solver=\"liblinear\",\n",
    "        class_weight=\"balanced\",   # robust to class imbalance\n",
    "        max_iter=MAX_ITER,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    clf.fit(X_std, y.to_numpy())\n",
    "\n",
    "    # ---- Contribution usefulness = |w_e * x_{s,e}| ----\n",
    "    w = clf.coef_.ravel()  # aligned with X columns\n",
    "    Xstd_df = pd.DataFrame(X_std, index=X.index, columns=X.columns)\n",
    "    contrib = (Xstd_df * w).abs()\n",
    "\n",
    "    # ---- Normalize to [0,1] ----\n",
    "    if NORMALIZE_PER_SUBJECT:\n",
    "        mins = contrib.min(axis=1)\n",
    "        denom = (contrib.max(axis=1) - mins + 1e-12)\n",
    "        usefulness = contrib.sub(mins, axis=0).div(denom, axis=0)\n",
    "    else:\n",
    "        lo, hi = contrib.values.min(), contrib.values.max()\n",
    "        usefulness = (contrib - lo) / (hi - lo + 1e-12)\n",
    "\n",
    "    out = usefulness.reset_index().melt(id_vars=\"subject\",\n",
    "                                        var_name=\"electrode\",\n",
    "                                        value_name=\"usefulness\")\n",
    "    out[\"usefulness\"] = out[\"usefulness\"].clip(0, 1).round(3)\n",
    "\n",
    "    OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out.to_csv(OUT_PATH, index=False)\n",
    "\n",
    "    print(f\"Wrote {OUT_PATH}  rows={len(out)}  subjects={out['subject'].nunique()}  electrodes={out['electrode'].nunique()}\")\n",
    "    print(\"Example:\")\n",
    "    print(out.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
